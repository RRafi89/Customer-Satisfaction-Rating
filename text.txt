import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv('train.csv')

df.head()

df.dtypes

df.nunique()

df.shape

df.info()

df.isnull().sum()

df.describe()

df.drop(columns=['id', 'Unnamed: 0'], inplace=True)

for col in df.columns:
    if df[col].nunique() < 10:
        print(f"Unique values in column {col}")
        print(df[col].unique())

from sklearn.preprocessing import LabelEncoder, OneHotEncoder

cat_cols = ['Type of Travel', 'Customer Type', 'Class', 'satisfaction']
oh_col = ['Gender']

encoding_dict = {}
le = LabelEncoder()
oh = OneHotEncoder(sparse_output=False)

for col in cat_cols:
    df[col] = le.fit_transform(df[col])
    encoding_dict[col] = dict(zip(le.classes_, range(len(le.classes_))))

encoded_data = oh.fit_transform(df[oh_col])
encoded_df = pd.DataFrame(encoded_data, columns=oh.get_feature_names_out(oh_col))
final_df = pd.concat([df.drop(columns=oh_col), encoded_df], axis=1)
df.drop(columns=['Gender'], inplace=True)

print("\nEncoding Mappings:")
for col, mapping in encoding_dict.items():
    print(f"{col}: {mapping}")

print("\nFinal DataFrame:")
final_df.head()

plt.figure(figsize=(15, 8)) 
sns.boxenplot(y=df['Arrival Delay in Minutes'], x=df['satisfaction'], hue=df['satisfaction'])
plt.title('Arrival Delay by Satisfaction Level')
plt.xlabel('Satisfaction Level')
plt.ylabel('Arrival Delay in Minutes')
plt.show()

plt.figure(figsize=(15, 8)) 
sns.boxenplot(y=df['Departure Delay in Minutes'], x=df['satisfaction'], hue=df['satisfaction'])
plt.title('Departure Delay by Satisfaction Level')
plt.xlabel('Satisfaction Level')
plt.ylabel('Departure Delay in Minutes')
plt.show()

plt.figure(figsize=(15, 8)) 
sns.boxenplot(y=df['Age'], x=df['satisfaction'], hue=df['satisfaction'])
plt.title('Age by Satisfaction Level')
plt.xlabel('Satisfaction Level')
plt.ylabel('Age')
plt.show()

features = df[['Arrival Delay in Minutes', 'Departure Delay in Minutes', 'Flight Distance', 'Age', 'satisfaction']]

plt.figure(figsize=(15, 10))
sns.scatterplot(data=features, x='Arrival Delay in Minutes', y='Departure Delay in Minutes', hue='satisfaction')
plt.title('Arrival Delay vs Departure Delay')
plt.show()

plt.figure(figsize=(15, 10))
sns.scatterplot(data=features, x='Arrival Delay in Minutes', y='Flight Distance', hue='satisfaction')
plt.title('Arrival Delay vs Departure Delay')
plt.show()

plt.figure(figsize=(15, 10))
sns.scatterplot(data=features, x='Departure Delay in Minutes', y='Flight Distance', hue='satisfaction')
plt.title('Arrival Delay vs Departure Delay')
plt.show()

plt.figure(figsize=(18, 12))
for i, column in enumerate(features.columns[:-1], 1):
    plt.subplot(2, 2, i)
    sns.boxplot(x='satisfaction', y=column, data=features, hue='satisfaction')
    plt.title(f'Boxplot of {column} by Satisfaction')
plt.tight_layout()
plt.show()

plt.figure(figsize=(18, 12))
for i, column in enumerate(features.columns[:-1], 1):
    plt.subplot(2, 2, i)
    sns.boxenplot(x='satisfaction', y=column, data=features, hue='satisfaction')
    plt.title(f'Violin Plot of {column} by Satisfaction')
plt.tight_layout()
plt.show()


plt.figure(figsize=(15, 8))
sns.countplot(x='satisfaction', data=features, hue='satisfaction')
plt.title('Count of Satisfaction Levels')
plt.show()


missing_values = features.isnull().sum()
print("Missing Values:\n", missing_values)

plt.figure(figsize=(15, 6))
sns.heatmap(features.isnull(), cbar=False, cmap='viridis')
plt.title('Missing Values Heatmap')
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

palette = sns.color_palette("Set2", n_colors=len(features.columns)-1)

plt.figure(figsize=(15, 10))
for i, column in enumerate(features.columns[:-1], 1):
    plt.subplot(2, 2, i)
    sns.histplot(features[column], kde=True, bins=30, color=palette[i-1], alpha=0.6)
    plt.title(f'Distribution of {column}', fontsize=16)
    plt.xlabel(column, fontsize=14)
    plt.ylabel('Frequency', fontsize=14)
    plt.grid(axis='y', linestyle='--', alpha=0.7)

plt.tight_layout()
plt.show()


# Pairplot with different kinds of plots
sns.pairplot(features, hue='satisfaction', diag_kind='kde', markers=["o", "o"])
plt.suptitle('Pairplot with KDE and Different Markers', y=1.02)
plt.show()


import seaborn as sns
import matplotlib.pyplot as plt

# Set a color palette
palette = sns.color_palette("Set1", n_colors=len(features.columns)-1)

# Outlier detection using IQR
Q1 = features.quantile(0.25)
Q3 = features.quantile(0.75)
IQR = Q3 - Q1

# Define outlier conditions
outlier_condition = (features < (Q1 - 1.5 * IQR)) | (features > (Q3 + 1.5 * IQR))

# Visualize outliers with enhanced aesthetics
plt.figure(figsize=(15, 10))
for i, column in enumerate(features.columns[:-1], 1):  # Exclude 'satisfaction'
    plt.subplot(2, 2, i)
    sns.boxplot(x=features[column], color=palette[i-1], fliersize=5, linewidth=1.5)
    plt.title(f'Boxplot of {column} with Outliers', fontsize=16)
    plt.xlabel(column, fontsize=14)
    plt.grid(axis='y', linestyle='--', alpha=0.7)  # Add grid for better readability

plt.tight_layout()
plt.show()


features = df[['Class', 'Type of Travel', 'Seat comfort', 'Inflight entertainment', 'Online boarding', 'Customer Type', 'Inflight wifi service', 'satisfaction']]

sns.pairplot(features, hue='satisfaction')

features = df[['Arrival Delay in Minutes', 'Departure Delay in Minutes', 'Flight Distance', 'Age', 'satisfaction']]

sns.pairplot(features, hue='satisfaction')

from itertools import combinations

def create_interactions(df, feature_list):
    for feat1, feat2 in combinations(feature_list, 2):
        df[f'{feat1}_x_{feat2}'] = df[feat1] * df[feat2]
    return df

def remove_collinearity_with_target(df, target_col, threshold=0.90):
    X = df.drop(columns=[target_col])
    target_corr = abs(X.corrwith(df[target_col]))
    corr_matrix = X.corr().abs()
    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
    to_drop = []
    
    for col in upper.columns:
        correlated_features = upper[col][upper[col] > threshold].index
        for feat in correlated_features:
            if target_corr[col] < target_corr[feat]:
                to_drop.append(col)
                break
    
    return df.drop(columns=to_drop)

df = df[[col for col in df.columns if col != 'satisfaction'] + ['satisfaction']]

corr = df.corr()
mask = np.triu(np.ones_like(corr, dtype=bool))
plt.figure(figsize=(15, 12))
sns.heatmap(corr, mask=mask, cmap='coolwarm', annot=True, fmt=".2f", square=True, cbar_kws={"shrink": .8})
plt.title('Correlation Matrix')
plt.show()

df.dropna(axis=0, inplace=True)

df.isnull().sum()

# import pandas as pd
# import shap
# import xgboost as xgb
# import matplotlib.pyplot as plt

# X = df.drop(columns=["satisfaction"])
# y = df["satisfaction"]
# X = pd.get_dummies(X, drop_first=True)

# model = xgb.XGBClassifier() if y.nunique() > 2 else xgb.XGBRegressor()
# model.fit(X, y)

# explainer = shap.Explainer(model, X)
# shap_values = explainer(X)

# plt.figure(figsize=(15, 10))
# shap.summary_plot(shap_values, X)

# plt.figure(figsize=(15, 10))
# shap.summary_plot(shap_values, X, plot_type="bar")

# plt.figure(figsize=(15, 10))
# shap.plots.waterfall(shap_values[0])

features = ['Class', 'Seat comfort', 'Inflight entertainment', 'Online boarding', 'Customer Type', 'Inflight wifi service']
df = create_interactions(df, features)

df = remove_collinearity_with_target(df, 'satisfaction', 0.85)

def remove_outliers(df, method="zscore", threshold=3):
    df_clean = df.copy()
    
    if method == "zscore":
        z_scores = np.abs((df_clean - df_clean.mean()) / df_clean.std())
        df_clean = df_clean[(z_scores < threshold).all(axis=1)]
    
    elif method == "iqr":
        Q1 = df_clean.quantile(0.25)
        Q3 = df_clean.quantile(0.75)
        IQR = Q3 - Q1
        df_clean = df_clean[~((df_clean < (Q1 - 1.5 * IQR)) | (df_clean > (Q3 + 1.5 * IQR))).any(axis=1)]
    
    elif method == "percentile":
        lower, upper = df_clean.quantile(0.01), df_clean.quantile(0.99)
        df_clean = df_clean[(df_clean >= lower) & (df_clean <= upper)].dropna()

    return df_clean


# df = remove_outliers(df, method="iqr")


def cap_outliers(df, method="zscore", threshold=3):
    df_capped = df.copy()

    if method == "zscore":
        z_scores = np.abs((df_capped - df_capped.mean()) / df_capped.std())
        df_capped[z_scores > threshold] = df_capped.mean() + np.sign(df_capped - df_capped.mean()) * df_capped.std() * threshold

    elif method == "iqr":
        Q1 = df_capped.quantile(0.25)
        Q3 = df_capped.quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        df_capped = df_capped.apply(lambda x: np.where(x < lower_bound[x.name], lower_bound[x.name], 
                                                        np.where(x > upper_bound[x.name], upper_bound[x.name], x)))

    elif method == "percentile":
        lower, upper = df_capped.quantile(0.01), df_capped.quantile(0.99)
        df_capped = df_capped.apply(lambda x: np.where(x < lower[x.name], lower[x.name], 
                                                        np.where(x > upper[x.name], upper[x.name], x)))

    return pd.DataFrame(df_capped, columns=df.columns)

# df = cap_outliers(df, method="iqr")

numerical_cols = df.select_dtypes(include=[np.number]).columns
skewed_cols = df[numerical_cols].apply(lambda x: x.skew()).abs()
skewed_cols = skewed_cols[skewed_cols > 0.5].index

def transform_skewed(df, method="log"):
    df_transformed = df.copy()
    
    for col in skewed_cols:
        if method == "log":
            df_transformed[col] = np.log1p(df_transformed[col])
        elif method == "sqrt":
            df_transformed[col] = np.sqrt(df_transformed[col])
        elif method == "boxcox":
            from scipy.stats import boxcox
            df_transformed[col], _ = boxcox(df_transformed[col] + 1)
    
    return df_transformed

df = transform_skewed(df, method="log")

from itertools import combinations


columns = ['Type of Travel', 'Inflight wifi service', 'Online boarding', 
           'Inflight service', 'Customer Type', 'Cleanliness', 
           'Inflight entertainment', 'Class', 'Seat comfort']



# Create interaction features
for feature1, feature2 in combinations(columns, 2):
    interaction_feature_name = f"{feature1}_x_{feature2}"
    df[interaction_feature_name] = df[feature1] * df[feature2]

X = df.drop(columns=['satisfaction'])
y = df['satisfaction']

def remove_high_collinearity(df, threshold=0.90):
    correlation_matrix = df.corr().abs()
    
    upper = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))

    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]
    
    df_reduced = df.drop(columns=to_drop)
    
    return df_reduced


from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import StandardScaler

def create_polynomial_features(X, degree=2, interaction_only=False):
    poly = PolynomialFeatures(degree=degree, interaction_only=interaction_only)
    X_poly = poly.fit_transform(X)
    
    feature_names = poly.get_feature_names_out(X.columns)
    X_poly_df = pd.DataFrame(X_poly, columns=feature_names)
    
    X_poly_df = X_poly_df.drop(columns=['1'])
    
    scaler = StandardScaler()
    X_poly_scaled = scaler.fit_transform(X_poly_df)
    X_poly_scaled_df = pd.DataFrame(X_poly_scaled, columns=X_poly_df.columns)
    
    return X_poly_scaled_df

# X_polynomial = create_polynomial_features(X, degree=2, interaction_only=False)
X = remove_high_collinearity(X, 0.85)

X.shape

import os
os.environ["LOKY_MAX_CPU_COUNT"] = "4"

import xgboost as xgb
import lightgbm as lgb
from catboost import CatBoostClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, r2_score

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

lgbm_params = {'n_estimators': 291, 'learning_rate': 0.033236472434062025, 'max_depth': 14, 'num_leaves': 73, 'min_child_samples': 15, 'subsample': 0.6452608030046519, 'colsample_bytree': 0.7351330578836698}
rf_params = {'n_estimators': 383, 'max_depth': 15, 'min_samples_split': 5, 'max_features': 'sqrt', 'criterion': 'entropy'}
cb_params = {'iterations': 485, 'learning_rate': 0.02451154055716572, 'depth': 8, 'l2_leaf_reg': 1.312879755477539}
xgb_params = {'max_depth': 15, 'learning_rate': 0.12204550084705108, 'n_estimators': 61, 'min_child_weight': 1.9190931783540948, 'subsample': 0.9329682809203258, 'colsample_bytree': 0.7196095681095573, 'gamma': 2.3019670579460234}

models = {
    'XGBoost': xgb.XGBClassifier(**xgb_params, random_state=42),
    'Random Forest': RandomForestClassifier(**rf_params, random_state=42),
    'LightGBM': lgb.LGBMClassifier(**lgbm_params, random_state=42, verbose=-1),
    'CatBoost': CatBoostClassifier(**cb_params, random_seed=42, verbose=False)
}

predictions = {}
for name, model in models.items():
    print(f"\n{'-'*50}")
    print(f"{name} Results:")
    model.fit(X_train, y_train)
    
    predictions[name] = model.predict(X_test)
    print(f"Accuracy: {accuracy_score(y_test, predictions[name]):.4f}")
    print(f"ROC AUC: {roc_auc_score(y_test, predictions[name]):.4f}")
    print(f"R2 Score: {r2_score(y_test, predictions[name]):.4f}")
    print("\nClassification Report:")
    print(classification_report(y_test, predictions[name], digits=3))


models_comparison = pd.DataFrame({
    'Model': list(models.keys()),
    'Accuracy': [accuracy_score(y_test, predictions[model]) for model in models.keys()],
    'ROC AUC': [roc_auc_score(y_test, predictions[model]) for model in models.keys()],
    'R2 score': [r2_score(y_test, predictions[model]) for model in models.keys()]
})
models_comparison = models_comparison.sort_values('Accuracy', ascending=False)

print("\n" + "="*50)
print("Models Comparison:")
print(models_comparison.to_string(index=False))

# import optuna
# from sklearn.model_selection import StratifiedKFold
# from sklearn.metrics import confusion_matrix

# X_train, X_test, y_train, y_test = train_test_split(
#     X, y, test_size=0.2, random_state=42
# )

# if hasattr(X_train, 'values'):
#     X_train_values = X_train.values
#     X_test_values = X_test.values
#     if hasattr(y_train, 'values'):
#         y_train_values = y_train.values
#         y_test_values = y_test.values
#     else:
#         y_train_values = y_train
#         y_test_values = y_test
# else:
#     X_train_values = X_train
#     X_test_values = X_test
#     y_train_values = y_train
#     y_test_values = y_test

# def objective(trial):
#     param = {
#         'max_depth': trial.suggest_int('max_depth', 3, 15),
#         'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.5),
#         'min_child_weight': trial.suggest_float('min_child_weight', 1, 20),
#         'subsample': trial.suggest_float('subsample', 0.6, 1.0),
#         'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
#         'gamma': trial.suggest_float('gamma', 0, 5),
#         'random_state': 42,
#         'objective': 'binary:logistic'
#     }
    
#     n_estimators = trial.suggest_int('n_estimators', 50, 1000)
#     skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
#     scores = []
    
#     for train_index, val_index in skf.split(X_train_values, y_train_values):
#         X_fold_train = X_train_values[train_index]
#         y_fold_train = y_train_values[train_index]
#         X_fold_val = X_train_values[val_index]
#         y_fold_val = y_train_values[val_index]
        
#         dtrain = xgb.DMatrix(X_fold_train, label=y_fold_train)
#         dval = xgb.DMatrix(X_fold_val, label=y_fold_val)
        
#         evallist = [(dval, 'validation')]
        
#         model = xgb.train(
#             params=param,
#             dtrain=dtrain,
#             num_boost_round=n_estimators,
#             evals=evallist,
#             early_stopping_rounds=20,
#             verbose_eval=False
#         )
        
#         y_pred_proba = model.predict(xgb.DMatrix(X_fold_val))
#         y_pred = (y_pred_proba > 0.5).astype(int)
#         acc = accuracy_score(y_fold_val, y_pred)
#         scores.append(acc)
    
#     return np.mean(scores)

# study = optuna.create_study(direction='maximize')
# study.optimize(objective, n_trials=100)

# print("Best parameters:", study.best_params)
# print("Best cross-validation accuracy:", study.best_value)

# best_params = study.best_params.copy()

# X_train_final, X_val, y_train_final, y_val = train_test_split(
#     X_train_values, y_train_values, test_size=0.2, random_state=42
# )

# dtrain_final = xgb.DMatrix(X_train_final, label=y_train_final)
# dval = xgb.DMatrix(X_val, label=y_val)
# dtest = xgb.DMatrix(X_test_values, label=y_test_values)

# print("Training final model with best parameters...")
# evallist = [(dtrain_final, 'train'), (dval, 'validation')]

# final_model = xgb.train(
#     params=best_params,
#     dtrain=dtrain_final,
#     num_boost_round=1000,
#     evals=evallist,
#     early_stopping_rounds=20,
#     verbose_eval=100
# )

# print(f"Best iteration: {final_model.best_iteration}")

# y_pred_proba = final_model.predict(dtest)
# y_pred = (y_pred_proba > 0.5).astype(int)

# print("Test set evaluation:")
# print("Accuracy:", accuracy_score(y_test_values, y_pred))
# print("\nClassification Report:")
# print(classification_report(y_test_values, y_pred))
# print("\nConfusion Matrix:")
# print(confusion_matrix(y_test_values, y_pred))
# print("\nROC AUC Score:", roc_auc_score(y_test_values, y_pred_proba))

# evals_result = {}
# final_model = xgb.train(
#     params=best_params,
#     dtrain=dtrain_final,
#     num_boost_round=1000,
#     evals=evallist,
#     early_stopping_rounds=20,
#     evals_result=evals_result,
#     verbose_eval=False
# )
# # This took 18 minutes to run


# plt.figure(figsize=(10, 6))
# plt.plot(evals_result['train']['error'] if 'error' in evals_result['train'] else evals_result['train']['rmse'], 
#          label='Training Error/Loss')
# plt.plot(evals_result['validation']['error'] if 'error' in evals_result['validation'] else evals_result['validation']['rmse'], 
#          label='Validation Error/Loss')
# plt.axvline(x=final_model.best_iteration, color='r', linestyle='--', 
#             label=f'Early Stopping Round: {final_model.best_iteration}')
# plt.xlabel('Number of Boosting Rounds')
# plt.ylabel('Error/Loss')
# plt.title('XGBoost Learning Curves with Early Stopping')
# plt.legend()
# plt.grid(True)
# plt.tight_layout()
# plt.show()

# plt.figure(figsize=(12, 6))
# xgb.plot_importance(final_model, max_num_features=15, importance_type='gain')
# plt.title('Feature Importance')
# plt.tight_layout()
# plt.show()

from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, log_loss, 
                             classification_report, roc_curve, precision_recall_curve)
from xgboost import XGBClassifier
from sklearn.model_selection import StratifiedKFold

params = {'max_depth': 15, 'learning_rate': 0.010455610252596594, 'min_child_weight': 1.3175484558283062, 'subsample': 0.9425350291622379, 'colsample_bytree': 0.7829601867423397, 'gamma': 3.320649090124465, 'n_estimators': 802}

model = XGBClassifier(**params)

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

accuracy_list, precision_list, recall_list, f1_list, roc_auc_list, logloss_list, r2_list = [], [], [], [], [], [], []

for train_idx, test_idx in cv.split(X, y):
    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]
    model.fit(X_train, y_train)

    y_pred_proba = model.predict_proba(X_test)[:, 1]
    y_pred = (y_pred_proba > 0.5).astype(int)
    accuracy_list.append(accuracy_score(y_test, y_pred))
    precision_list.append(precision_score(y_test, y_pred))
    recall_list.append(recall_score(y_test, y_pred))
    f1_list.append(f1_score(y_test, y_pred))
    roc_auc_list.append(roc_auc_score(y_test, y_pred_proba))
    logloss_list.append(log_loss(y_test, y_pred_proba))
    r2_list.append(r2_score(y_test, y_pred))

print("Cross-Validation Results:")
print(f"Accuracy: {np.mean(accuracy_list):.4f} ± {np.std(accuracy_list):.4f}")
print(f"Precision: {np.mean(precision_list):.4f} ± {np.std(precision_list):.4f}")
print(f"Recall: {np.mean(recall_list):.4f} ± {np.std(recall_list):.4f}")
print(f"F1 Score: {np.mean(f1_list):.4f} ± {np.std(f1_list):.4f}")
print(f"ROC AUC: {np.mean(roc_auc_list):.4f} ± {np.std(roc_auc_list):.4f}")
print(f"Log Loss: {np.mean(logloss_list):.4f} ± {np.std(logloss_list):.4f}")
print(f"R2 score: {np.mean(r2_list):.4f} ± {np.std(r2_list):.4f}")

fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
plt.figure(figsize=(7, 5))
plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc_list[-1]:.2f})', color='blue')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.show()

precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)
plt.figure(figsize=(7, 5))
plt.plot(recall, precision, label='Precision-Recall Curve', color='green')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend()
plt.show()

# from sklearn.model_selection import cross_val_score

# cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# def objective_rf(trial):
#     params = {
#         'n_estimators': trial.suggest_int('n_estimators', 100, 500),
#         'max_depth': trial.suggest_int('max_depth', 3, 15),
#         'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),
#         'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2']),
#         'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy'])
#     }
    
#     model = RandomForestClassifier(**params, random_state=42, n_jobs=-1)
#     scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='roc_auc', n_jobs=-1)
#     return np.mean(scores)

# study_rf = optuna.create_study(direction='maximize')
# study_rf.optimize(objective_rf, n_trials=50)

# rf_model = RandomForestClassifier(**study_rf.best_params, random_state=42, n_jobs=-1)
# rf_model.fit(X_train, y_train)
# rf_pred = rf_model.predict(X_test)

# print("Random Forest Best Params:", study_rf.best_params)
# print("Accuracy:", accuracy_score(y_test, rf_pred))
# print("ROC AUC:", roc_auc_score(y_test, rf_pred))
# print("Classification Report:\n", classification_report(y_test, rf_pred))

# def objective_lgb(trial):
#     params = {
#         'objective': 'binary',
#         'metric': 'auc',
#         'n_estimators': trial.suggest_int('n_estimators', 100, 500),
#         'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
#         'max_depth': trial.suggest_int('max_depth', 3, 15),
#         'num_leaves': trial.suggest_int('num_leaves', 10, 100),
#         'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),
#         'subsample': trial.suggest_float('subsample', 0.5, 1.0),
#         'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0)
#     }
    
#     model = lgb.LGBMClassifier(**params, random_state=42)
#     scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='roc_auc', n_jobs=-1)
#     return np.mean(scores)

# study_lgb = optuna.create_study(direction='maximize')
# study_lgb.optimize(objective_lgb, n_trials=50)

# lgb_model = lgb.LGBMClassifier(**study_lgb.best_params, random_state=42)
# lgb_model.fit(X_train, y_train)
# lgb_pred = lgb_model.predict(X_test)

# print("LightGBM Best Params:", study_lgb.best_params)
# print("Accuracy:", accuracy_score(y_test, lgb_pred))
# print("ROC AUC:", roc_auc_score(y_test, lgb_pred))
# print("Classification Report:\n", classification_report(y_test, lgb_pred))


# def objective_cat(trial):
#     params = {
#         'iterations': trial.suggest_int('iterations', 100, 500),
#         'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
#         'depth': trial.suggest_int('depth', 3, 10),
#         'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-3, 10.0, log=True)
#     }
    
#     model = CatBoostClassifier(**params, random_seed=42, verbose=False)
#     scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='roc_auc', n_jobs=-1)
#     return np.mean(scores)

# study_cat = optuna.create_study(direction='maximize')
# study_cat.optimize(objective_cat, n_trials=50)

# cat_model = CatBoostClassifier(**study_cat.best_params, random_seed=42, verbose=False)
# cat_model.fit(X_train, y_train)
# cat_pred = cat_model.predict(X_test)

# print("CatBoost Best Params:", study_cat.best_params)
# print("Accuracy:", accuracy_score(y_test, cat_pred))
# print("ROC AUC:", roc_auc_score(y_test, cat_pred))
# print("Classification Report:\n", classification_report(y_test, cat_pred))
# # This code took 72 minutes to run

